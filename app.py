"""
AI Dashboard Assistant (Offline, Streamlit)
File: ai_dashboard_assistant_streamlit.py

Description:
A self-contained Streamlit app that accepts CSV/Excel uploads, performs automatic
analysis and visualization, generates rule-based natural-language insights, and
allows basic natural-language style queries (interpreted to safe pandas operations).
It does NOT use any external paid APIs. Optional local LLM hooks (Hugging Face)
are provided as comments for future extension.

Requirements:
pip install streamlit pandas numpy plotly scikit-learn python-multipart
# Optional (for local LLM integration):
# pip install transformers accelerate torch

Run:
streamlit run ai_dashboard_assistant_streamlit.py

Notes:
- The app focuses on being offline-friendly and safe. No external network calls are made.
- The "AI" textual insights are generated by rule-based heuristics. You can uncomment
  the local-LM example and provide a compatible local model if you have one.

"""

import streamlit as st
import pandas as pd
import numpy as np
import io
import base64
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder
import plotly.express as px
import plotly.graph_objects as go
from textwrap import dedent

# Optional local model example (commented)
# from transformers import pipeline
# local_generator = pipeline("text-generation", model="meta-llama/Llama-2-7b-chat-hf")

st.set_page_config(page_title="AI Dashboard Assistant (Offline)", layout="wide")

# -------------------- Helper functions --------------------

def read_file(uploaded_file):
    try:
        if uploaded_file.name.endswith('.csv'):
            return pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith(('.xls', '.xlsx')):
            return pd.read_excel(uploaded_file)
        else:
            st.error("Unsupported file type. Upload CSV or Excel files.")
            return None
    except Exception as e:
        st.error(f"Failed to read file: {e}")
        return None


def safe_describe(df):
    # produce a compact describe that won't error on mixed dtypes
    num = df.select_dtypes(include=[np.number])
    cat = df.select_dtypes(include=['object', 'category', 'bool'])
    desc = {
        'shape': df.shape,
        'num_columns': list(num.columns),
        'cat_columns': list(cat.columns),
        'missing_values': df.isnull().sum().to_dict(),
        'head': df.head(5).to_dict(orient='records')
    }
    return desc


def generate_rule_based_insights(df, max_insights=10):
    insights = []
    n, m = df.shape
    insights.append(f"Dataset has {n} rows and {m} columns.")

    # Missing values
    miss = df.isnull().sum()
    miss = miss[miss > 0].sort_values(ascending=False)
    for col, val in miss.items():
        insights.append(f"Column '{col}' has {val} missing values ({val/n:.1%} of rows).")
        if len(insights) >= max_insights: return insights

    # Numeric stats and outliers
    numeric = df.select_dtypes(include=[np.number])
    if not numeric.empty:
        desc = numeric.describe().T
        # high variance
        high_var = desc[desc['std'] > desc['mean'] * 0.5]
        for idx in high_var.index[:3]:
            insights.append(f"Numeric column '{idx}' shows high variability (std={desc.loc[idx,'std']:.2f}).")
            if len(insights) >= max_insights: return insights

        # correlations
        corr = numeric.corr()
        # find strong correlations (>0.7)
        strong_pairs = []
        for i, c1 in enumerate(corr.columns):
            for j, c2 in enumerate(corr.columns):
                if j <= i: continue
                val = corr.iloc[i, j]
                if abs(val) >= 0.7:
                    strong_pairs.append((c1, c2, val))
        strong_pairs = sorted(strong_pairs, key=lambda x: -abs(x[2]))
        for c1, c2, val in strong_pairs[:3]:
            insights.append(f"Strong correlation ({val:.2f}) between '{c1}' and '{c2}'.")
            if len(insights) >= max_insights: return insights

        # outlier detection (IQR)
        for col in numeric.columns[:5]:
            series = numeric[col].dropna()
            if series.empty: continue
            q1, q3 = series.quantile(0.25), series.quantile(0.75)
            iqr = q3 - q1
            if iqr == 0: continue
            lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr
            outliers = series[(series < lower) | (series > upper)]
            if not outliers.empty:
                insights.append(f"Column '{col}' has {len(outliers)} potential outliers (IQR method).")
                if len(insights) >= max_insights: return insights

    # Categorical frequency
    cat = df.select_dtypes(include=['object', 'category', 'bool'])
    for col in cat.columns[:3]:
        top = df[col].value_counts(dropna=True).nlargest(3)
        top_list = ", ".join([f"{idx}({cnt})" for idx, cnt in top.items()])
        insights.append(f"Column '{col}' top values: {top_list}.")
        if len(insights) >= max_insights: return insights

    # default message
    if len(insights) == 1:
        insights.append("No major issues detected from quick heuristics. Consider deeper modelling or manual review.")

    return insights


def df_to_markdown_report(df, insights, charts_md=None):
    # Compose a markdown report that can be downloaded
    md = ["# AI Dashboard Assistant Report\n"]
    md.append(f"**Rows:** {df.shape[0]}  ")
    md.append(f"**Columns:** {df.shape[1]}  \n")

    md.append("## Insights\n")
    for i, ins in enumerate(insights, 1):
        md.append(f"{i}. {ins}  \n")

    if charts_md:
        md.append("## Charts\n")
        md.extend(charts_md)

    return "\n".join(md)

# -------------------- Streamlit layout --------------------

st.title("AI Dashboard Assistant (Offline)")
st.write("Upload a CSV/XLSX file. The app will analyze it, create visualizations, and generate rule-based insights â€” no external API required.")

uploaded_file = st.file_uploader("Upload CSV or Excel file", type=['csv', 'xls', 'xlsx'])

if uploaded_file is not None:
    df = read_file(uploaded_file)
    if df is None:
        st.stop()

    st.sidebar.header("Data overview")
    desc = safe_describe(df)
    st.sidebar.write(f"Shape: {desc['shape']}")
    st.sidebar.write(f"Numeric columns ({len(desc['num_columns'])}): {', '.join(desc['num_columns'][:10])}")
    st.sidebar.write(f"Categorical columns ({len(desc['cat_columns'])}): {', '.join(desc['cat_columns'][:10])}")

    # show head
    st.subheader("Data sample")
    st.dataframe(df.head(10))

    # Quick cleaning options
    st.sidebar.subheader("Quick cleaning")
    fill_method = st.sidebar.selectbox("Fill numeric missing with", ['none', 'mean', 'median', 'zero'])
    drop_na_thresh = st.sidebar.slider("Drop columns with >% missing", 0, 100, 100)

    # apply cleaning
    working = df.copy()
    if drop_na_thresh < 100:
        thresh = int((drop_na_thresh / 100.0) * len(working))
        working = working.dropna(axis=1, thresh=thresh)
    if fill_method != 'none':
        num_cols = working.select_dtypes(include=[np.number]).columns
        if fill_method == 'mean':
            working[num_cols] = working[num_cols].fillna(working[num_cols].mean())
        elif fill_method == 'median':
            working[num_cols] = working[num_cols].fillna(working[num_cols].median())
        elif fill_method == 'zero':
            working[num_cols] = working[num_cols].fillna(0)

    st.subheader("Auto Visualizations")
    num_cols = working.select_dtypes(include=[np.number]).columns.tolist()
    cat_cols = working.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    col1, col2 = st.columns([2,1])
    with col1:
        st.write("### Numeric distributions")
        if num_cols:
            sel = st.selectbox("Choose numeric column for histogram", num_cols)
            fig = px.histogram(working, x=sel, nbins=30, marginal='box')
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No numeric columns found.")

    with col2:
        st.write("### Categorical summary")
        if cat_cols:
            scel = st.selectbox("Choose categorical column for bar chart", cat_cols)
            top = working[scel].value_counts().nlargest(10).reset_index()
            top.columns = [scel, 'count']
            fig2 = px.bar(top, x=scel, y='count')
            st.plotly_chart(fig2, use_container_width=True)
        else:
            st.info("No categorical columns found.")

    st.write("### Correlation heatmap (numeric)")
    if len(num_cols) >= 2:
        corr = working[num_cols].corr()
        fig3 = px.imshow(corr, text_auto=True)
        st.plotly_chart(fig3, use_container_width=True)
    else:
        st.info("Need at least 2 numeric columns for correlation.")

    # Generate insights
    st.subheader("Generated Insights")
    insights = generate_rule_based_insights(working, max_insights=12)
    for i, ins in enumerate(insights, 1):
        st.markdown(f"**{i}.** {ins}")

    # Optional: refine insights with a local LLM (commented)
    if st.checkbox("(Optional) Refine insights with local LLM (requires models & heavy resources)"):
        st.warning("Make sure you have a compatible local model and enough memory. This example is a placeholder and does not run by default.")
        # Example prompt composition (do not run unless models are available):
        prompt = """
        You are a data analyst. Given the dataset summary and initial insights, produce clearer, concise recommendations.

        Dataset head: {head}
        Initial insights: {insights}
        """.format(head=working.head(5).to_string(), insights='; '.join(insights))
        st.code(prompt[:1000] + '...')
        # Uncomment and adapt if you have local generator
        # with st.spinner('Generating text from local model...'):
        #     out = local_generator(prompt, max_new_tokens=256)
        #     st.write(out[0]['generated_text'])

    # Simple natural-language-ish query box -> Safe interpreter
    st.subheader("Ask a question (safe, limited)")
    user_q = st.text_input("E.g., 'top 10 rows where sales > 1000' or 'show correlation between age and salary'")
    if user_q:
        # Very simple parsing rules
        q = user_q.lower()
        try:
            if q.startswith('top') and 'rows' in q:
                # parse 'top 10 rows where ...'
                import re
                m = re.search(r'top (\d+) rows where (.+)', q)
                if m:
                    k = int(m.group(1))
                    cond = m.group(2)
                    # restrict allowed tokens to prevent arbitrary eval
                    allowed = set('abcdefghijklmnopqrstuvwxyz0123456789_ ><=.\"\'"()+-*/%')
                    if any(ch not in allowed and not ch.isspace() for ch in cond):
                        st.error('Condition contains disallowed characters.')
                    else:
                        # replace column names with df['col'] style if present
                        safe_cond = cond
                        for col in working.columns:
                            if col.lower() in cond:
                                safe_cond = safe_cond.replace(col.lower(), f"`{col}`")
                        # use query with backticks
                        res = working.query(cond).head(k)
                        st.dataframe(res)
                else:
                    st.info("Couldn't parse query. Try format: 'top 10 rows where <condition>'")
            elif 'correlation' in q or 'correlate' in q:
                # parse 'correlation between a and b'
                import re
                m = re.search(r'correlation between (.+) and (.+)', q)
                if m:
                    a = m.group(1).strip()
                    b = m.group(2).strip()
                    # find matching columns (case-insensitive)
                    cols_lower = {c.lower(): c for c in working.columns}
                    if a in cols_lower and b in cols_lower:
                        val = working[cols_lower[a]].corr(working[cols_lower[b]])
                        st.write(f"Correlation between {cols_lower[a]} and {cols_lower[b]}: {val:.3f}")
                    else:
                        st.error('Could not match columns. Use exact column names as in the table.')
                else:
                    st.info("Try: 'correlation between <col1> and <col2>'")
            else:
                st.info('Query not recognized by the simple interpreter. Try the example formats.')
        except Exception as e:
            st.error(f"Failed to run query: {e}")

    # Report composition and download
    st.subheader("Export Report")
    charts_md = []
    charts_md.append("- Auto visualizations shown in the dashboard.")
    md = df_to_markdown_report(working, insights, charts_md)
    st.download_button("Download markdown report", md, file_name='ai_dashboard_report.md')

    # Download cleaned data
    buf = io.BytesIO()
    working.to_csv(buf, index=False)
    st.download_button("Download cleaned CSV", buf.getvalue(), file_name='cleaned_data.csv')

else:
    st.info("Upload a CSV or Excel file to begin.")

# Footer
st.markdown("---")
st.write("Tips: For large datasets, consider sampling before upload. To add richer natural-language insights, run a local LLM and adapt the prompt in the placeholder area.")
